# 实验三：朴素贝叶斯算法实验

#### 【实验目的】

理解朴素贝叶斯算法原理，掌握朴素贝叶斯算法框架。

#### 【实验内容】

1. 针对下表中的数据，编写python程序实现朴素贝叶斯算法（不使用sklearn包），对输入数据进行预测；
2. 熟悉sklearn库中的朴素贝叶斯算法，使用sklearn包编写朴素贝叶斯算法程序，对输入数据进行预测；

#### 【实验报告要求】

1. 对照实验内容，撰写实验过程、算法及测试结果；
2. 代码规范化：命名规则、注释；
3. 查阅文献，讨论朴素贝叶斯算法的应用场景。



| 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 好瓜 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 碍滑 | 是   |
| 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 碍滑 | 是   |
| 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 碍滑 | 是   |
| 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 碍滑 | 是   |
| 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 碍滑 | 是   |
| 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 是   |
| 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 | 是   |
| 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 是   |
| 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 否   |
| 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 | 否   |
| 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 否   |
| 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 | 否   |
| 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 | 否   |
| 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 | 否   |
| 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 否   |
| 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 | 否   |
| 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 否   |



#### 1.编写python程序实现朴素贝叶斯算法（不使用sklearn包）：

对已下数据进行预测：

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| 测1  | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |  ？  |

1. 导入数据：

```python
# 导入数据
import pandas as pd

df = pd.read_csv("../data/data_word.csv")
df
```

输出：

|      | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 好瓜 |
| ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
|    0 | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |   是 |
|    1 | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 |   是 |
|    2 | 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |   是 |
|    3 | 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 |   是 |
|    4 | 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |   是 |
|    5 | 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 |   是 |
|    6 | 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 |   是 |
|    7 | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 |   是 |
|    8 | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 |   否 |
|    9 | 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 |   否 |
|   10 | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 |   否 |
|   11 | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 |   否 |
|   12 | 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 |   否 |
|   13 | 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 |   否 |
|   14 | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 |   否 |
|   15 | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 |   否 |
|   16 | 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 |   否 |

2. 处理数据：

```python
data = df.values[:, 1:-1]
test = df.values[0,1:-1]
labels = df.values[:,-1].tolist()
```

3. 计算先验概率：

$$
P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}
$$

代码：

```python
prob_good = log((8 + 1) / float(17 + 2))
prob_bad = log((9 + 1) / float(17 + 2))
```

4. 计算条件概率:

$$
P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}
$$

代码：

```python
count_good = 0
count_bad = 0
for column in range(len(data)):
	if test[i] == data[column,i]:
		if labels[column] == 1:
			count_good += 1
		if labels[column] == 0:
			count_bad += 1
prob_good += log(float(count_good + 1) / (8 + class_number(i)))
prob_bad += log(float(count_bad + 1) / (9 + class_number(i)))
```

5. 完整代码：

```python
# python3
# -- coding: utf-8 --
# -------------------------------
# @Author : LiMinG
# @Email : 2168884970@qq.com
# -------------------------------
# @File : 实验三：朴素贝叶斯算法实验(不使用Skearn).py
# @Software : PyCharm
# @Time : 2022/11/11 20:03
# -------------------------------

import numpy as np
import pandas as pd


def Y_prob(Y):  # 好瓜和坏瓜的各自的比重
    y = Y.values
    true_prob = sum(y) / len(y)
    return {0: 1 - true_prob, 1: true_prob}


def x_y_prob(feature, y):  # 求p(特征|类别)，用字典的形式存储返回
    x_y = {}
    n = len(y)
    for f in feature.columns:
        # for i in range(feature_num(feature[f])): #x_f = i
        for i in feature[f].value_counts().keys():
            index = (y == 1)
            index_True = y[y == True].index
            index_False = y[y == False].index

            sample_True = data.loc[index_True, f] == i
            prob_true = len(sample_True.loc[sample_True == True]) / len(index_True)
            strings_True = str(f) + '=' + str(i) + '|' + 'y=1'

            sample_False = data.loc[index_False, f] == i
            prob_False = len(sample_False.loc[sample_False == True]) / len(index_False)
            strings_False = str(f) + '=' + str(i) + '|' + 'y=0'

            x_y[strings_True] = prob_true
            x_y[strings_False] = prob_False
    return x_y


if __name__ == '__main__':
    data = pd.read_csv('E:\MyCode\机器学习\data\data_word.csv')
    feature = data.columns.values[:-1]
    x_y = x_y_prob(data[feature], data['好瓜'])  # 用字典存储p(特征|类别)
    y_prob = Y_prob(data['好瓜'])
    # 要预测的数据
    test_data = ['青绿', '蜷缩', '沉闷', '稍糊', '凹陷', '硬滑']
    # 将测试数据转化为字典可以搜索的格式
    test_true = [str(feature[i]) + '=' + str(test_data[i]) + '|' + 'y=1' for i in range(len(feature))]
    test_false = [str(feature[i]) + '=' + str(test_data[i]) + '|' + 'y=0' for i in range(len(feature))]
    p_true = y_prob[1]
    p_false = y_prob[0]
    for i in range(len(feature)):  # 计算每一类的概率
        p_true *= x_y[test_true[i]]
        p_false *= x_y[test_false[i]]
    print('特征为：{}的瓜，是好瓜的概率为：{:.8f}，不是好瓜的概率为：{:.8f}'.format(test_data, p_true, p_false))
    print('朴素贝叶斯最后预测：' + '是好瓜' if p_true > p_false else '不是好瓜')
```

输出预测结果：

```
特征为：['青绿', '蜷缩', '沉闷', '稍糊', '凹陷', '硬滑']的瓜，是好瓜的概率为：0.00161564，不是好瓜的概率为：0.00129105
朴素贝叶斯最后预测：是好瓜
```



#### 2.使用sklearn包编写朴素贝叶斯算法程序：

```python
# python3
# -- coding: utf-8 --
# -------------------------------
# @Author : LiMinG
# @Email : 2168884970@qq.com
# -------------------------------
# @File : 实验三：朴素贝叶斯算法实验(使用sklearn).py
# @Software : PyCharm
# @Time : 2022/11/11 20:15
# -------------------------------

# 输入数据集
datasets1 = [['0', '0', '0', '0', '0', '0', '1'],
             ['1', '0', '1', '0', '0', '0', '1'],
             ['1', '0', '0', '0', '0', '0', '1'],
             ['0', '0', '1', '0', '0', '0', '1'],
             ['2', '0', '0', '0', '0', '0', '1'],
             ['0', '1', '0', '0', '1', '1', '1'],
             ['1', '1', '0', '1', '1', '1', '1'],
             ['1', '1', '0', '0', '1', '2', '1'],
             ['1', '1', '1', '1', '1', '2', '0'],
             ['0', '2', '2', '0', '2', '1', '0'],
             ['2', '2', '2', '2', '2', '2', '0'],
             ['2', '0', '0', '2', '2', '1', '0'],
             ['0', '1', '0', '1', '0', '2', '0'],
             ['2', '1', '1', '1', '0', '2', '0'],
             ['1', '1', '0', '0', '1', '1', '0'],
             ['2', '0', '0', '2', '2', '2', '0'],
             ['0', '0', '1', '1', '1', '2', '0']
             ]
# 青绿:0 乌黑:1 浅白:2
# 蜷缩 0 稍蜷 1 硬挺 2
# 浊响 0 沉闷 1 清脆 2
# 清晰 0 稍糊 1 模糊 2
# 凹陷 0 稍凹 1 平坦 2
# 碍滑 0 软粘 1 硬滑 2
# 是 1 否 0
labels = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']

# 样本数据
import pandas as pd

# 将数据集转换为DataFrame数据
data1 = pd.DataFrame(datasets1, columns=labels)

from sklearn.model_selection import train_test_split  # 将原始数据划分为数据集与测试集两个部分
from sklearn.naive_bayes import BernoulliNB

X = data1.iloc[:, :-1]
y = data1.iloc[:, -1]
# X_train训练样本, X_test测试样本, y_train训练样本分类, y_test测试样本分类
# X样本数据分类集, y分类结果集, test_size=3测试样本数量,random_state=1 生成数据随机
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=3, random_state=None)

clf = BernoulliNB()
clf.fit(X, y)
# 返回预测的精确性
clf.score(X_test, y_test)
# 查看预测结果
clf.predict(X_test)
# 输入测试样本 ['青绿','蜷缩','浊响','清晰','凹陷','硬滑']
tt = ['0', '0', '0', '0', '0', '2']
tt = pd.DataFrame(tt)
test = tt.T
# 查看预测结果
print(clf.predict(test))
```

输出结果：

```
['1']
```

通过调用sklearn包对样本['青绿','蜷缩','浊响','清晰','凹陷','硬滑']进行预测的结果是**好瓜**。



#### 3.查阅文献，讨论朴素贝叶斯算法的应用场景。

**优点：**

1. 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率；
2. 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已；
3. 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）；
4. 对缺失数据不太敏感，算法也比较简单，常用于文本分类；
5. 朴素贝叶斯对结果解释容易理解。

**缺点：**

1. 需要计算先验概率；
2. 分类决策存在错误率；
3. 对输入数据的表达形式很敏感；
4. 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

**朴素贝叶斯应用领域：**

1. 欺诈检测中使用较多；
2. 一封电子邮件是否是垃圾邮件；
3. 一篇文章应该分到科技、政治，还是体育类；
4. 一段文字表达的是积极的情绪还是消极的情绪；
5. 人脸识别。
